{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393a682d-0dfe-476f-a172-10efb79549dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PASO 1: Variables de configuracion: rutas a landing, raw y bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc1f2ce-d5b2-48c9-92a5-d8fce9715be4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf, regexp_extract\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec5234c-5ee9-4066-bb66-f837aca4b21a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "account = spark.conf.get(\"adls.account.name\")\n",
    "\n",
    "landing_container = f\"abfss://landing@{account}.dfs.core.windows.net\"\n",
    "lakehouse_container = f\"abfss://lakehouse@{account}.dfs.core.windows.net\"\n",
    "\n",
    "landing_path = landing_container\n",
    "raw_path = f\"{lakehouse_container}/raw\"\n",
    "bronze_path = f\"{lakehouse_container}/bronze\"\n",
    "\n",
    "print(landing_path)\n",
    "print(raw_path)\n",
    "print(bronze_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f19ac6a0-b613-4757-b6e9-ba1c7b54b8b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PASO 2: Definimos funcion de utilidad que muestras todos los ficheros de un directorio\n",
    "\n",
    "Si hay subdirectorios, incluye los ficheros presenetes en estos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53736c27-150d-4485-b5d3-3340cfb31ecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_all_files(path:str):\n",
    "    \"\"\"\n",
    "    Lista recursivamente todos los archivos en una ruta y sus subcarpetas utilizando dbutils.fs.\n",
    "    \n",
    "    Parámetros:\n",
    "        path (str): Ruta base en el sistema de archivos (e.g., 'dbfs:/mnt/mi_carpeta').\n",
    "    \n",
    "    Retorna:\n",
    "        list: Lista de rutas completas de todos los archivos encontrados.\n",
    "    \"\"\"\n",
    "    files_list = []\n",
    "    try:\n",
    "        # Listar contenidos de la ruta actual\n",
    "        items = dbutils.fs.ls(path)\n",
    "        \n",
    "        for item in items:\n",
    "            # Si es un archivo, añadirlo a la lista\n",
    "            if item.isFile():\n",
    "                files_list.append(item.path)\n",
    "            # Si es un directorio, explorar recursivamente\n",
    "            elif item.isDir():\n",
    "                files_list.extend(list_all_files(item.path))\n",
    "    except Exception as e:\n",
    "        print(f\"Error al listar {path}: {str(e)}\")\n",
    "    \n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42ae0d8-a419-43a6-9bdc-be1da30f3a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PASO 3: Creamos una clase que permite leer los ficheros de landing, para tipos de ficheros tanto json como jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "492944c5-7e75-4681-838d-e299e386a7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ANTIGUO\n",
    "def extract_label(path_col):\n",
    "    \"\"\"Extract label from file path using built-in SQL functions.\"\"\"\n",
    "    return regexp_extract(path_col, \"flower_photos/([^/]+)\", 1)\n",
    "    \n",
    "   \n",
    "def extract_size(content):\n",
    "    \"\"\"Extract image size from its raw content.\"\"\"\n",
    "    image = Image.open(io.BytesIO(content))\n",
    "    return image.size\n",
    "    \n",
    "    \n",
    "@pandas_udf(\"width: int, height: int\")\n",
    "def extract_size_udf(content_series):\n",
    "    sizes = content_series.apply(LandingStreamReader.extract_size)\n",
    "    return pd.DataFrame(list(sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ffb88ae6-393e-43c3-b9d2-8f68d7a09307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ANTIGUO\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, replace,lit\n",
    "\n",
    "class LandingStreamReader:\n",
    "\n",
    "    IMAGE_EXTENSIONS = [\n",
    "        \"jpg\", \"jpeg\", \"png\", \"bmp\", \"gif\", \"tiff\", \"tif\",\n",
    "        \"webp\", \"heic\", \"heif\", \"ico\", \"svg\", \"raw\", \"cr2\",\n",
    "        \"nef\", \"orf\", \"arw\", \"psd\", \"indd\", \"ai\", \"eps\"\n",
    "    ]\n",
    "\n",
    "    IMAGE_GENERAL_KEYWORD = \"image\"\n",
    "\n",
    "    def __init__(self, builder):\n",
    "        self.datasource = builder.datasource\n",
    "        self.dataset = builder.dataset\n",
    "        self.landing_path = builder.landing_path\n",
    "        self.raw_path = builder.raw_path\n",
    "        self.bronze_path = builder.bronze_path\n",
    "        self.format = builder.format\n",
    "        self.dataset_landing_path = f'{self.landing_path}/{self.datasource}/{self.dataset}'\n",
    "        self.dataset_bronze_schema_location = f'{self.bronze_path}/{self.datasource}/{self.dataset}_schema'\n",
    "        dbutils.fs.mkdirs(self.dataset_bronze_schema_location)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (f\"LandingStreamReader(datasource='{self.datasource}',dataset='{self.dataset}')\")\n",
    "        \n",
    "    def add_metadata_columns(self,df):\n",
    "        data_cols = df.columns\n",
    "      \n",
    "        metadata_cols = {\n",
    "            \"_ingested_at\": current_timestamp(),\n",
    "            \"_ingested_filename\": replace(\n",
    "                                    input_file_name(),\n",
    "                                    lit(self.landing_path),\n",
    "                                    lit(self.raw_path) )\n",
    "        }\n",
    "\n",
    "        if (self.format in LandingStreamReader.IMAGE_EXTENSIONS) or (self.format == LandingStreamReader.IMAGE_GENERAL_KEYWORD):\n",
    "            metadata_cols.update({\n",
    "                \"_size\":extract_size_udf(col(\"content\")),\n",
    "                \"_label\":extract_label(col(\"path\")),\n",
    "            })\n",
    "\n",
    "        for col_name, expr in metadata_cols.items():\n",
    "            df = df.withColumn(col_name, expr) \n",
    "      \n",
    "      #reordernamos columnas\n",
    "        return df.select(list(metadata_cols.keys()) + data_cols)  \n",
    "    \n",
    "    def read_json(self):\n",
    "      return (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"json\")\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.schemaLocation\", self.dataset_bronze_schema_location)\n",
    "            .load(self.dataset_landing_path)\n",
    "        )\n",
    "\n",
    "    def read_binary(self):\n",
    "        return (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\",\"binaryFile\")\n",
    "            # las imagenes vienen en subdirectorios con la etiqueta.\n",
    "            .option(\"recursiveFileLookup\", \"true\")\n",
    "            .option(\"cloudFiles.schemaLocation\", self.dataset_bronze_schema_location)\n",
    "            .load(self.dataset_landing_path)\n",
    "\n",
    "        )\n",
    "   \n",
    "    \n",
    "    def read(self):\n",
    "        \n",
    "        df = None\n",
    "\n",
    "        match self.format:\n",
    "            case \"json\":\n",
    "                df = self.read_json()\n",
    "            case x if x in LandingStreamReader.IMAGE_EXTENSIONS or x == LandingStreamReader.IMAGE_GENERAL_KEYWORD:\n",
    "                df = self.read_binary()\n",
    "            case _:\n",
    "                raise Exception(f\"Format {self.format} not supported\")\n",
    "\n",
    "        if df:\n",
    "            df = df.transform(self.add_metadata_columns)\n",
    "            return df\n",
    "    \n",
    "    class Builder:\n",
    "        def __init__(self):\n",
    "            self.datasource = None\n",
    "            self.dataset = None\n",
    "            self.landing_path = None\n",
    "            self.raw_path = None\n",
    "            self.bronze_path = None\n",
    "            self.format = None\n",
    "        \n",
    "        def set_datasource(self, datasource):\n",
    "            self.datasource = datasource\n",
    "            return self\n",
    "        \n",
    "        def set_dataset(self, dataset):\n",
    "            self.dataset = dataset\n",
    "            return self\n",
    "        \n",
    "        def set_landing_path(self, landing_path):\n",
    "            self.landing_path = landing_path\n",
    "            return self\n",
    "        \n",
    "        def set_raw_path(self, raw_path):\n",
    "            self.raw_path = raw_path\n",
    "            return self\n",
    "        \n",
    "        def set_bronze_path(self, bronze_path):\n",
    "            self.bronze_path = bronze_path\n",
    "            return self\n",
    "          \n",
    "        def set_format(self, format):\n",
    "            self.format = format\n",
    "            return self\n",
    "        \n",
    "        def build(self):\n",
    "            return LandingStreamReader(self)\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de98267-9af5-49fe-8e14-524491dacf49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "import io\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, input_file_name, replace, regexp_extract, pandas_udf\n",
    ")\n",
    "\n",
    "# Función auxiliar: extraer tamaño de imagen\n",
    "def extract_size(content):\n",
    "    image = Image.open(io.BytesIO(content))\n",
    "    return image.size\n",
    "\n",
    "# UDF para extraer (width, height)\n",
    "@pandas_udf(\"width: int, height: int\")\n",
    "def extract_size_udf(content_series):\n",
    "    sizes = content_series.apply(extract_size)\n",
    "    return pd.DataFrame(list(sizes))\n",
    "\n",
    "# Extraer etiqueta desde el path\n",
    "def extract_label(path_col):\n",
    "    return regexp_extract(path_col, \"flower_photos/([^/]+)\", 1)\n",
    "\n",
    "# Añadir columnas de metadatos\n",
    "def add_metadata_columns(df, landing_path, raw_path, format, image_extensions, image_keyword):\n",
    "    data_cols = df.columns\n",
    "\n",
    "    metadata_cols = {\n",
    "        \"_ingested_at\": current_timestamp(),\n",
    "        \"_ingested_filename\": replace(\n",
    "            input_file_name(),\n",
    "            lit(landing_path),\n",
    "            lit(raw_path)\n",
    "        )\n",
    "    }\n",
    "\n",
    "    if format in image_extensions or format == image_keyword:\n",
    "        metadata_cols.update({\n",
    "            \"_size\": extract_size_udf(col(\"content\")),\n",
    "            \"_label\": extract_label(col(\"path\")),\n",
    "        })\n",
    "\n",
    "    for col_name, expr in metadata_cols.items():\n",
    "        df = df.withColumn(col_name, expr)\n",
    "\n",
    "    return df.select(list(metadata_cols.keys()) + [c for c in data_cols if c not in metadata_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ccd899-489b-43ec-a2a9-ad609b95dcc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class LandingStreamReader:\n",
    "\n",
    "    IMAGE_EXTENSIONS = [\n",
    "        \"jpg\", \"jpeg\", \"png\", \"bmp\", \"gif\", \"tiff\", \"tif\",\n",
    "        \"webp\", \"heic\", \"heif\", \"ico\", \"svg\", \"raw\", \"cr2\",\n",
    "        \"nef\", \"orf\", \"arw\", \"psd\", \"indd\", \"ai\", \"eps\"\n",
    "    ]\n",
    "\n",
    "    IMAGE_GENERAL_KEYWORD = \"image\"\n",
    "\n",
    "    def __init__(self, builder):\n",
    "        self.datasource = builder.datasource\n",
    "        self.dataset = builder.dataset\n",
    "        self.landing_path = builder.landing_path\n",
    "        self.raw_path = builder.raw_path\n",
    "        self.bronze_path = builder.bronze_path\n",
    "        self.format = builder.format\n",
    "        self.dataset_landing_path = f'{self.landing_path}/{self.datasource}/{self.dataset}'\n",
    "        self.dataset_bronze_schema_location = f'{self.bronze_path}/{self.datasource}/{self.dataset}_schema'\n",
    "        dbutils.fs.mkdirs(self.dataset_bronze_schema_location)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"LandingStreamReader(datasource='{self.datasource}', dataset='{self.dataset}')\"\n",
    "\n",
    "    def read_json(self):\n",
    "        return (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"json\")\n",
    "                .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                .option(\"cloudFiles.schemaLocation\", self.dataset_bronze_schema_location)\n",
    "                .load(self.dataset_landing_path))\n",
    "\n",
    "    def read_binary(self):\n",
    "        return (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"binaryFile\")\n",
    "                .option(\"recursiveFileLookup\", \"true\")\n",
    "                .option(\"cloudFiles.schemaLocation\", self.dataset_bronze_schema_location)\n",
    "                .load(self.dataset_landing_path))\n",
    "\n",
    "    def read(self):\n",
    "        match self.format:\n",
    "            case \"json\":\n",
    "                df = self.read_json()\n",
    "            case x if x in LandingStreamReader.IMAGE_EXTENSIONS or x == LandingStreamReader.IMAGE_GENERAL_KEYWORD:\n",
    "                df = self.read_binary()\n",
    "            case _:\n",
    "                raise Exception(f\"Format {self.format} not supported\")\n",
    "\n",
    "        return add_metadata_columns(\n",
    "            df,\n",
    "            self.landing_path,\n",
    "            self.raw_path,\n",
    "            self.format,\n",
    "            LandingStreamReader.IMAGE_EXTENSIONS,\n",
    "            LandingStreamReader.IMAGE_GENERAL_KEYWORD\n",
    "        )\n",
    "\n",
    "    class Builder:\n",
    "        def __init__(self):\n",
    "            self.datasource = None\n",
    "            self.dataset = None\n",
    "            self.landing_path = None\n",
    "            self.raw_path = None\n",
    "            self.bronze_path = None\n",
    "            self.format = None\n",
    "\n",
    "        def set_datasource(self, datasource):\n",
    "            self.datasource = datasource\n",
    "            return self\n",
    "\n",
    "        def set_dataset(self, dataset):\n",
    "            self.dataset = dataset\n",
    "            return self\n",
    "\n",
    "        def set_landing_path(self, landing_path):\n",
    "            self.landing_path = landing_path\n",
    "            return self\n",
    "\n",
    "        def set_raw_path(self, raw_path):\n",
    "            self.raw_path = raw_path\n",
    "            return self\n",
    "\n",
    "        def set_bronze_path(self, bronze_path):\n",
    "            self.bronze_path = bronze_path\n",
    "            return self\n",
    "\n",
    "        def set_format(self, format):\n",
    "            self.format = format\n",
    "            return self\n",
    "\n",
    "        def build(self):\n",
    "            return LandingStreamReader(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5ad0f58-30ed-481c-9660-c966fc58ea63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PASO 4: Clase de escritura, reutilizada del ejercicio anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8de50d0c-e559-408b-91cc-4743cc86cacd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name, replace,lit\n",
    "\n",
    "class BronzeStreamWriter:   \n",
    "    def __init__(self, builder):\n",
    "        self.datasource = builder.datasource\n",
    "        self.dataset = builder.dataset\n",
    "        self.landing_path = builder.landing_path\n",
    "        self.raw_path = builder.raw_path\n",
    "        self.bronze_path = builder.bronze_path\n",
    "        self.dataset_landing_path = f\"{self.landing_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_raw_path =  f\"{self.raw_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_bronze_path = f\"{self.bronze_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_checkpoint_location = f'{dataset_bronze_path}_checkpoint'\n",
    "        self.table = f'hive_metastore.bronze.{self.datasource}_{self.dataset}'\n",
    "        self.query_name = f\"bronze-{datasource}-{dataset}\"\n",
    "        dbutils.fs.mkdirs(self.dataset_raw_path)\n",
    "        dbutils.fs.mkdirs(self.dataset_bronze_path)\n",
    "        dbutils.fs.mkdirs(self.dataset_checkpoint_location)\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"BronzeStreamWriter(datasource='{self.datasource}',dataset='{self.dataset}')\")\n",
    "         \n",
    "    def archive_raw_files(self,df):\n",
    "      if \"_ingested_filename\" in df.columns:\n",
    "        files = [row[\"_ingested_filename\"] for row in df.select(\"_ingested_filename\").distinct().collect()]\n",
    "        for file in files:\n",
    "          if file:\n",
    "              file_landing_path = file.replace(self.dataset_raw_path,self.dataset_landing_path)\n",
    "              dbutils.fs.mkdirs(file[0:file.rfind('/')+1])\n",
    "              dbutils.fs.mv(file_landing_path,file)\n",
    "    \n",
    "    def write_data(self,df):\n",
    "      spark.sql( 'CREATE DATABASE IF NOT EXISTS hive_metastore.bronze') \n",
    "      spark.sql(f\"CREATE TABLE IF NOT EXISTS {self.table} USING DELTA LOCATION '{self.dataset_bronze_path}' \") \n",
    "      (df.write\n",
    "          .format(\"delta\")  \n",
    "          .mode(\"append\")\n",
    "          .option(\"mergeSchema\", \"true\")\n",
    "          .option(\"path\", self.dataset_bronze_path)\n",
    "          .saveAsTable(self.table)\n",
    "      )\n",
    "        \n",
    "    def append_2_bronze(self,batch_df, batch_id):\n",
    "      batch_df.persist()\n",
    "      self.write_data(batch_df)\n",
    "      self.archive_raw_files(batch_df)\n",
    "      batch_df.unpersist()\n",
    "      \n",
    "\n",
    "    class Builder:\n",
    "        def __init__(self):\n",
    "            self.datasource = None\n",
    "            self.dataset = None\n",
    "            self.landing_path = None\n",
    "            self.raw_path = None\n",
    "            self.bronze_path = None\n",
    "        \n",
    "        def set_datasource(self, datasource):\n",
    "            self.datasource = datasource\n",
    "            return self\n",
    "        \n",
    "        def set_dataset(self, dataset):\n",
    "            self.dataset = dataset\n",
    "            return self\n",
    "        \n",
    "        def set_landing_path(self, landing_path):\n",
    "            self.landing_path = landing_path\n",
    "            return self\n",
    "        \n",
    "        def set_raw_path(self, raw_path):\n",
    "            self.raw_path = raw_path\n",
    "            return self\n",
    "        \n",
    "        def set_bronze_path(self, bronze_path):\n",
    "            self.bronze_path = bronze_path\n",
    "            return self\n",
    "        \n",
    "        def build(self):\n",
    "            return BronzeStreamWriter(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5761eb-16e7-4f8c-9223-76f3f106345c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "format = \"jpg\"\n",
    "datasource = 'tensorflow'\n",
    "dataset = \"flower_photos\"\n",
    "\n",
    "#format = \"json\"\n",
    "#datasource = 'retail'\n",
    "#dataset = \"sales_orders\"\n",
    "\n",
    "dataset_landing_path = f\"{landing_path}/{datasource}/{dataset}\"\n",
    "dataset_raw_path =  f\"{raw_path}/{datasource}/{dataset}\"\n",
    "dataset_bronze_path = f\"{bronze_path}/{datasource}/{dataset}\"\n",
    "\n",
    "print(dataset_landing_path)\n",
    "print(dataset_raw_path)\n",
    "print(dataset_bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1b402b-cab7-4343-9916-790bc4a07d86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reader = (LandingStreamReader.Builder()          \n",
    "  .set_datasource(datasource)\n",
    "  .set_dataset(dataset)\n",
    "  .set_landing_path(landing_path)\n",
    "  .set_raw_path(raw_path)\n",
    "  .set_bronze_path(bronze_path)\n",
    "  .set_format(format)\n",
    "  .build()\n",
    ")\n",
    "\n",
    "print(reader)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4baccd-e003-45f3-8226-5d2fd7f42a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98d40bef-ed5b-4719-8358-36249be46bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(reader.read().limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74978d49-dcd4-481e-ad27-e60cbb2a82ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "writer = (BronzeStreamWriter.Builder()\n",
    "  .set_datasource(datasource)\n",
    "  .set_dataset(dataset)\n",
    "  .set_landing_path(landing_path)\n",
    "  .set_raw_path(raw_path)\n",
    "  .set_bronze_path(bronze_path)\n",
    "  .build()\n",
    ")\n",
    "\n",
    "print(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746251bc-e435-446f-b377-f580df1c3c70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(reader\n",
    "  .read()\n",
    "  .writeStream\n",
    "  .foreachBatch(writer.append_2_bronze)\n",
    "  .trigger(availableNow=True)\n",
    "  #.trigger(processingTime=\"60 seconds\") # modo continuo\n",
    "  .option(\"checkpointLocation\", writer.dataset_checkpoint_location)\n",
    "  .queryName(writer.query_name)\n",
    "  .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3263ec25-8bc5-42e4-9c59-5931d2d16076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select * \n",
    "from delta.`{writer.dataset_bronze_path}`\n",
    "order by _ingested_at desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "display(spark.sql(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0948d009-b8f3-4db0-b537-d201030ac4f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select distinct _ingested_filename \n",
    "from delta.`{writer.dataset_bronze_path}`\n",
    "\"\"\"\n",
    "display(spark.sql(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b69ef231-a0ba-4917-aee6-cd68e1e57078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "prueba_inicial",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
