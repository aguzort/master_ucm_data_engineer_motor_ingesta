{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f280fe21-64d2-4af7-abff-35e0df186152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../imports/imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71fa3335-659a-42c1-aff4-9a05e92bfe71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config/paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a48fb73-c123-4ec0-96cc-5e105d23de9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../utilities/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d2e632-fbcd-4879-9b6e-564921ed029c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(landing_path)\n",
    "print(raw_path)\n",
    "print(bronze_path)\n",
    "print(kafka_config_path)\n",
    "print(kafka_schema_registry_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fecc4502-db01-4d26-84d4-1797c50a0994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class LandingStreamReader:\n",
    "\n",
    "    ENVIRONMENT_ATTRIBUTES = [\"landing_path\", \"raw_path\", \"bronze_path\"]    \n",
    "\n",
    "    KAFKA_CONFIG_ATTRIBUTES = [\"kafka_config\", \"kafka_schema_registry_config\"]\n",
    "\n",
    "    DATASET_INGESTION_ATTRIBUTES = [\"datasource\", \"dataset\", \"source_format\", \"source_options\", \"metadata_columns\", \"partition_column\", \"formatted_date_column_params\"]\n",
    "\n",
    "    KAFKA_INGESTION_ATTRIBUTES = [\"kafka_key_schema\", \"kafka_value_schema\", \"kafka_flatten_value\", \"kafka_key_serializer\", \"kafka_value_serializer\" ]\n",
    "\n",
    "    ATTRIBUTES = [\n",
    "        *ENVIRONMENT_ATTRIBUTES,\n",
    "        *KAFKA_CONFIG_ATTRIBUTES,\n",
    "        *DATASET_INGESTION_ATTRIBUTES,\n",
    "        *KAFKA_INGESTION_ATTRIBUTES\n",
    "    ]\n",
    "\n",
    "    def __init__(self, builder):\n",
    "\n",
    "        # Attributes setted via builder\n",
    "        for attribute in LandingStreamReader.ATTRIBUTES:\n",
    "            setattr(self, attribute, getattr(builder, attribute))\n",
    "            \n",
    "        # Calculated attributes\n",
    "        self.dataset_landing_path = f'{self.landing_path}/{self.datasource}/{self.dataset}'\n",
    "        self.dataset_bronze_schema_location = f'{self.bronze_path}/{self.datasource}/{self.dataset}_schema'\n",
    "        \n",
    "        # Create schema location directory\n",
    "        dbutils.fs.mkdirs(self.dataset_bronze_schema_location)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"LandingStreamReader(datasource='{self.datasource}', dataset='{self.dataset}')\"\n",
    "\n",
    "    def read_cloudFiles(self):\n",
    "        df = (spark.readStream\n",
    "              .format(\"cloudFiles\")\n",
    "              .options(**self.source_options)\n",
    "              .option(\"cloudFiles.schemaLocation\", self.dataset_bronze_schema_location)\n",
    "              .load(self.dataset_landing_path)\n",
    "        )\n",
    "\n",
    "        df = add_metadata_columns(\n",
    "            df,\n",
    "            self.source_format,\n",
    "            self.landing_path,\n",
    "            self.raw_path,\n",
    "            self.metadata_columns\n",
    "        )\n",
    "\n",
    "        if self.partition_column and self.formatted_date_column_params:\n",
    "            df = add_formatted_date_column(df,self.partition_column, **self.formatted_date_column_params)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def read_kafka(self):\n",
    "        df=(spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .options(**self.kafka_config)\n",
    "            .options(**self.source_options)\n",
    "            .load()\n",
    "        )\n",
    "\n",
    "        columns = [F.col(column).alias(f'_{column}') for column in df.columns]\n",
    "        \n",
    "        df = df.select(*columns)\n",
    "\n",
    "        df = deserialize_df(\n",
    "            df = df,\n",
    "            value_serializer=self.kafka_value_serializer,\n",
    "            key_serializer=self.kafka_key_serializer,\n",
    "            topic = self.dataset,\n",
    "            schema_registry_config = self.kafka_schema_registry_config,\n",
    "            flatten_value = self.kafka_flatten_value,\n",
    "            value_schema = self.kafka_value_schema,\n",
    "            key_schema = self.kafka_key_schema\n",
    "        ) \n",
    "\n",
    "        df = add_metadata_columns(\n",
    "            df,\n",
    "            format = \"kafka\",\n",
    "        )\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def read(self):\n",
    "        if self.source_format == \"cloudFiles\":\n",
    "            return self.read_cloudFiles()\n",
    "        elif self.source_format == \"kafka\":\n",
    "            return self.read_kafka()\n",
    "        else:\n",
    "            raise Exception(f\"Format {self.source_format} not supported\")\n",
    "\n",
    "    class Builder:\n",
    "        def __init__(self):\n",
    "            for attribute in LandingStreamReader.ATTRIBUTES:\n",
    "                setattr(self, attribute, None)\n",
    "\n",
    "        @classmethod\n",
    "        def _create_setters(cls):\n",
    "            for attribute in LandingStreamReader.ATTRIBUTES:\n",
    "                def setter(self, value, attribute=attribute):\n",
    "                    setattr(self, attribute, value)\n",
    "                    return self\n",
    "                setattr(cls, f\"set_{attribute}\", setter)            \n",
    "\n",
    "        def build(self):\n",
    "            return LandingStreamReader(self)\n",
    "        \n",
    "LandingStreamReader.Builder._create_setters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9295622-bc09-4a5d-b8d8-0b3407c07610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class BronzeStreamWriter:   \n",
    "\n",
    "    ENVIRONMENT_ATTRIBUTES = [\"landing_path\", \"raw_path\", \"bronze_path\"] \n",
    "\n",
    "    BRONZE_CONFIG_ATTRIBUTES = [\"bronze_table_format\", \"bronze_write_mode\"]   \n",
    "\n",
    "    DATASET_INGESTION_ATTRIBUTES = [\"datasource\", \"dataset\", \"sink_options\", \"partition_column\", \"formatted_date_column_params\"]\n",
    "\n",
    "    TRIGGER_ATTRIBUTES = [\"trigger_mode\"]\n",
    "\n",
    "    ATTRIBUTES = [\n",
    "        *ENVIRONMENT_ATTRIBUTES,\n",
    "        *BRONZE_CONFIG_ATTRIBUTES,\n",
    "        *DATASET_INGESTION_ATTRIBUTES,\n",
    "        *TRIGGER_ATTRIBUTES\n",
    "    ]\n",
    "\n",
    "    def __init__(self, builder):\n",
    "\n",
    "        # Attributes setted via builder\n",
    "        for attribute in BronzeStreamWriter.ATTRIBUTES:\n",
    "            setattr(self, attribute, getattr(builder, attribute))\n",
    "\n",
    "        # Calculated attributes\n",
    "        self.dataset_landing_path = f\"{self.landing_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_raw_path = f\"{self.raw_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_bronze_path = f\"{self.bronze_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_checkpoint_location = f\"{self.dataset_bronze_path}_checkpoint\"\n",
    "        self.table = f\"hive_metastore.bronze.{self.datasource}__{self.dataset}\"\n",
    "        self.query_name = f\"bronze-{self.datasource}-{self.dataset}\"\n",
    "\n",
    "        dbutils.fs.mkdirs(self.dataset_raw_path)\n",
    "        dbutils.fs.mkdirs(self.dataset_bronze_path)\n",
    "        dbutils.fs.mkdirs(self.dataset_checkpoint_location)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"BronzeStreamWriter(datasource='{self.datasource}', dataset='{self.dataset}')\"\n",
    "         \n",
    "    def archive_raw_files(self, df):\n",
    "        \"\"\"\n",
    "        Moves ingested raw files from landing to raw path after processing.\n",
    "        \"\"\"\n",
    "        if \"_ingested_filename\" in df.columns:\n",
    "            files = [row[\"_ingested_filename\"] for row in df.select(\"_ingested_filename\").distinct().collect()]\n",
    "            for file in files:\n",
    "                if file:\n",
    "                    file_landing_path = file.replace(self.dataset_raw_path, self.dataset_landing_path)\n",
    "                    dbutils.fs.mkdirs(file[0:file.rfind('/')+1])\n",
    "                    dbutils.fs.mv(file_landing_path, file)\n",
    "    \n",
    "    def write_data(self, df):\n",
    "        \"\"\"\n",
    "        Writes DataFrame to Delta table in bronze layer with schema merge and Delta Lake support.\n",
    "        \"\"\"\n",
    "        spark.sql(\"CREATE DATABASE IF NOT EXISTS hive_metastore.bronze\") \n",
    "        #spark.sql(f\"CREATE TABLE IF NOT EXISTS {self.table} USING DELTA LOCATION '{self.dataset_bronze_path}'\") \n",
    "        \n",
    "        writer = (\n",
    "            df.write\n",
    "           .format(self.bronze_table_format)\n",
    "           .mode(self.bronze_write_mode)\n",
    "           .options(**self.sink_options)\n",
    "        )\n",
    "\n",
    "        if self.partition_column and not self.formatted_date_column_params:\n",
    "            writer.partitionBy(self.partition_column)\n",
    "\n",
    "        if self.partition_column and self.formatted_date_column_params:\n",
    "            writer.partitionBy(self.formatted_date_column_params[\"output_col\"])\n",
    "        \n",
    "        (writer\n",
    "        .option(\"path\", self.dataset_bronze_path)\n",
    "        .saveAsTable(self.table)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def append_2_bronze(self, batch_df, batch_id):\n",
    "        \"\"\"\n",
    "        Main entrypoint for Structured Streaming write logic.\n",
    "        Persists, writes, archives, and unpersists the batch DataFrame.\n",
    "        \"\"\"\n",
    "        batch_df.persist()\n",
    "        self.write_data(batch_df)\n",
    "        self.archive_raw_files(batch_df)\n",
    "        batch_df.unpersist()\n",
    "\n",
    "    class Builder:\n",
    "        def __init__(self):\n",
    "            for attribute in BronzeStreamWriter.ATTRIBUTES:\n",
    "                setattr(self, attribute, None)\n",
    "\n",
    "        @classmethod\n",
    "        def _create_setters(cls):\n",
    "            for attribute in BronzeStreamWriter.ATTRIBUTES:\n",
    "                def setter(self, value, attribute=attribute):\n",
    "                    setattr(self, attribute, value)\n",
    "                    return self\n",
    "                setattr(cls, f\"set_{attribute}\", setter)            \n",
    "\n",
    "        def build(self):\n",
    "            return BronzeStreamWriter(self)\n",
    "        \n",
    "BronzeStreamWriter.Builder._create_setters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff138c24-c8e0-42d6-a8c5-a906d8944b1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_landing_stream_reader_from_json(\n",
    "    json_config_path: str,\n",
    "    landing_path: str = None,\n",
    "    raw_path: str = None,\n",
    "    bronze_path: str = None,\n",
    "    kafka_config_path: str = None,\n",
    "    kafka_schema_registry_config_path: str =None\n",
    "    ) -> LandingStreamReader: \n",
    "\n",
    "    VALID_INGESTION_ATTRIBUTES = [\n",
    "        *LandingStreamReader.DATASET_INGESTION_ATTRIBUTES,\n",
    "        *LandingStreamReader.KAFKA_INGESTION_ATTRIBUTES\n",
    "    \n",
    "    ]\n",
    "\n",
    "    reader = LandingStreamReader.Builder()\n",
    "\n",
    "    reader = (reader\n",
    "              .set_landing_path(landing_path)\n",
    "              .set_raw_path(raw_path)\n",
    "              .set_bronze_path(bronze_path)\n",
    "            )\n",
    "\n",
    "    if kafka_config_path:\n",
    "        kafka_config = get_kafka_config(read_config_file(kafka_config_path))\n",
    "        reader.set_kafka_config(kafka_config)\n",
    "\n",
    "    if kafka_schema_registry_config_path:\n",
    "        kafka_schema_registry_config = ( \n",
    "            get_schema_registry_config(read_config_file(kafka_schema_registry_config_path)) \n",
    "        )\n",
    "        reader.set_kafka_schema_registry_config(kafka_schema_registry_config)\n",
    "\n",
    "    with open(json_config_path) as json_file:\n",
    "        config = json.load(json_file)\n",
    "\n",
    "    for key, value in config.items():\n",
    "        if key in VALID_INGESTION_ATTRIBUTES:\n",
    "            setter_name = f\"set_{key}\"\n",
    "            setter = getattr(reader, setter_name)  \n",
    "            setter(value)  \n",
    "\n",
    "    return reader.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5f22506-16cd-45a6-a48f-3241970bdb6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_bronze_stream_writer_from_json(\n",
    "    bronze_config_path: str,\n",
    "    json_config_path: str,\n",
    "    landing_path: str = None,\n",
    "    raw_path: str = None,\n",
    "    bronze_path: str = None,\n",
    "    ) -> BronzeStreamWriter: \n",
    "\n",
    "    VALID_INGESTION_ATTRIBUTES = [\n",
    "        *BronzeStreamWriter.DATASET_INGESTION_ATTRIBUTES,\n",
    "        *BronzeStreamWriter.TRIGGER_ATTRIBUTES\n",
    "    \n",
    "    ]\n",
    "\n",
    "    VALID_BRONZE_ATTRIBUTES = BronzeStreamWriter.BRONZE_CONFIG_ATTRIBUTES\n",
    "\n",
    "    writer = BronzeStreamWriter.Builder()\n",
    "\n",
    "    writer = (writer\n",
    "              .set_landing_path(landing_path)\n",
    "              .set_raw_path(raw_path)\n",
    "              .set_bronze_path(bronze_path)\n",
    "            )\n",
    "    \n",
    "    with open(bronze_config_path) as bronze_config_file:\n",
    "        bronze_config = json.load(bronze_config_file)\n",
    "\n",
    "    for key, value in bronze_config.items():\n",
    "        if key in VALID_BRONZE_ATTRIBUTES:\n",
    "            setter_name = f\"set_{key}\"\n",
    "            setter = getattr(writer, setter_name)  \n",
    "            setter(value) \n",
    "\n",
    "    with open(json_config_path) as json_file:\n",
    "        config = json.load(json_file)\n",
    "\n",
    "    for key, value in config.items():\n",
    "        if key in VALID_INGESTION_ATTRIBUTES:\n",
    "            setter_name = f\"set_{key}\"\n",
    "            setter = getattr(writer, setter_name)  \n",
    "            setter(value)  \n",
    "\n",
    "    return writer.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b531fe6c-1a62-4bda-b04c-f90c58d86e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def start_ingestion_given_json(\n",
    "    bronze_config_path: str,\n",
    "    json_config_path: str,\n",
    "    landing_path: str = None,\n",
    "    raw_path: str = None,\n",
    "    bronze_path: str = None,\n",
    "    kafka_config_path: str = None,\n",
    "    kafka_schema_registry_config_path: str =None\n",
    "    ) -> None:\n",
    "\n",
    "    reader = build_landing_stream_reader_from_json(\n",
    "        json_config_path = json_config_path,\n",
    "        landing_path = landing_path,\n",
    "        raw_path = raw_path,\n",
    "        bronze_path = bronze_path,\n",
    "        kafka_config_path = kafka_config_path,\n",
    "        kafka_schema_registry_config_path = kafka_schema_registry_config_path\n",
    "    )\n",
    "\n",
    "    writer = build_bronze_stream_writer_from_json(\n",
    "        bronze_config_path = bronze_config_path,\n",
    "        json_config_path = json_config_path,\n",
    "        landing_path = landing_path,\n",
    "        raw_path = raw_path,\n",
    "        bronze_path = bronze_path,\n",
    "    )\n",
    "\n",
    "    (reader\n",
    "        .read()\n",
    "        .writeStream\n",
    "        .foreachBatch(writer.append_2_bronze)\n",
    "        .trigger(**writer.trigger_mode)\n",
    "        .option(\"checkpointLocation\", writer.dataset_checkpoint_location)\n",
    "        .queryName(writer.query_name)\n",
    "        .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6106bc24-1d93-48a7-958c-68f8867de587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_ingestion_given_json(\n",
    "    bronze_config_path = bronze_config_path,\n",
    "    json_config_path = \"../config/datasets/batch/retail_sales_order_v2.json\",\n",
    "    landing_path = landing_path,\n",
    "    raw_path = raw_path,\n",
    "    bronze_path = bronze_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df275a13-fd5c-4eeb-9025-f32dcbe4e010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_ingestion_given_json(\n",
    "    bronze_config_path = bronze_config_path,\n",
    "    json_config_path = \"../config/datasets/batch/orders_v3.json\",\n",
    "    landing_path = landing_path,\n",
    "    raw_path = raw_path,\n",
    "    bronze_path = bronze_path,\n",
    "    kafka_config_path  = kafka_config_path,\n",
    "    kafka_schema_registry_config_path = kafka_schema_registry_config_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e93c61a7-80f3-490d-aa64-02bf3856ed4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_sales_orders_writer = build_bronze_stream_writer_from_json(\n",
    "    json_config_path = \"../config/datasets/batch/retail_sales_order_v2.json\",\n",
    "    landing_path = landing_path,\n",
    "    raw_path = raw_path,\n",
    "    bronze_path = bronze_path,\n",
    "    kafka_config_path  = kafka_config_path,\n",
    "    kafka_schema_registry_config_path = kafka_schema_registry_config_path\n",
    ")\n",
    "\n",
    "print(test_sales_orders_writer)\n",
    "\n",
    "test_sales_orders_writer.sink_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16cbea69-ab3d-4437-a9f2-9cbed626cd34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_sales_orders = build_landing_stream_reader_from_json(\n",
    "    json_config_path = \"../config/datasets/batch/retail_sales_order_v2.json\",\n",
    "    landing_path = landing_path,\n",
    "    raw_path = raw_path,\n",
    "    bronze_path = bronze_path\n",
    ")\n",
    "\n",
    "print(test_sales_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36032c6d-9bf5-4372-8843-8cfc4cfc982e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_orders_v3 = build_landing_stream_reader_from_json(\n",
    "    json_config_path = \"../config/datasets/batch/orders_v3.json\",\n",
    "    landing_path = landing_path,\n",
    "    raw_path = raw_path,\n",
    "    bronze_path = bronze_path,\n",
    "    kafka_config_path  = kafka_config_path,\n",
    "    kafka_schema_registry_config_path = kafka_schema_registry_config_path\n",
    ")\n",
    "\n",
    "print(test_orders_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc8fcbc6-4025-430b-8356-b61ed8b62efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(test_orders_v3.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b777331a-ebca-465e-90ac-ac78082622c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "  datasource = \"retail\"\n",
    "  dataset =  \"sales_orders\"\n",
    "  source_format  = \"cloudFiles\"\n",
    "  source_options =  {\n",
    "                      \"cloudFiles.format\": \"json\",\n",
    "                      \"cloudFiles.inferColumnTypes\": \"true\",\n",
    "                      \"cloudFiles.schemaEvolutionMode\": \"addNewColumns\"\n",
    "                    }\n",
    "  metadata_columns =  \"default\"\n",
    "  partition_column = \"order_datetime\"\n",
    "  formatted_date_column_params=  {\n",
    "                                    \"output_col\": \"order_yyyymm\",\n",
    "                                    \"input_type\": \"unix\",\n",
    "                                    \"input_format\": None,\n",
    "                                    \"output_format\": \"yyyy-MM\"\n",
    "                                  }\n",
    "  sink_layer =  \"bronze\"\n",
    "  sink_options = {\n",
    "                    \"mergeSchema\": \"true\"\n",
    "                  }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a00cce4-58f8-447b-82b8-a1ecbb44bd76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datasource = \"pizzerie\"\n",
    "dataset = \"orders_v2\"\n",
    "source_format = \"kafka\"\n",
    "source_options = {\n",
    "    \"subscribe\": \"orders_v2\",\n",
    "    \"includeHeaders\": \"true\",\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}\n",
    "metadata_columns = \"default\"\n",
    "partition_column = \"key\"\n",
    "kafka_flatten_value = \"True\"\n",
    "kafka_key_serializer = \"str\"\n",
    "kafka_value_serializer = \"avro\"\n",
    "sink_layer = \"bronze\"\n",
    "sink_options = {\n",
    "    \"mergeSchema\": \"true\"\n",
    "}\n",
    "\n",
    "kafka_config = get_kafka_config(read_config_file('../config/kafka/client.properties'))\n",
    "kafka_schema_registry_config = get_schema_registry_config(read_config_file('../config/kafka/schema_registry.properties'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6acecb-68a2-43cf-8208-6a00b97cf368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reader_orders_v2 = (\n",
    "    LandingStreamReader.Builder()          \n",
    "    .set_landing_path(landing_path)\n",
    "    .set_raw_path(raw_path)\n",
    "    .set_bronze_path(bronze_path)\n",
    "    .set_datasource(datasource)\n",
    "    .set_dataset(dataset)\n",
    "    .set_source_format(source_format)\n",
    "    .set_source_options(source_options)\n",
    "    .set_metadata_columns(metadata_columns)\n",
    "    .set_partition_column(partition_column)\n",
    "    .set_kafka_flatten_value(kafka_flatten_value)\n",
    "    .set_kafka_key_serializer(kafka_key_serializer)\n",
    "    .set_kafka_value_serializer(kafka_value_serializer)\n",
    "    .set_kafka_config(kafka_config)\n",
    "    .set_kafka_schema_registry_config(kafka_schema_registry_config)\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f3a6bce-2b8b-429b-8f64-6f02d7972188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reader_orders_v2.source_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d606357-b19e-40c0-b99f-f214c3967dd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reader_orders_v2.source_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8948e850-3748-4226-834e-439128e568f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = reader_orders_v2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "804914bc-5a3e-44fa-8494-e020ad7723f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77076220-02f1-47fb-8212-f993d7fa0c3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_path = '../config/datasets/batch/orders.json'\n",
    "\n",
    "with open(json_path) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config.get('source').get('options').get('cloudFiles.format')\n",
    "\n",
    "bronze_config_path = '../config/bronze.json'\n",
    "with open(bronze_config_path) as f:\n",
    "    bronze_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d376ec27-114d-4422-8d59-3ae244cc6362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#format = \"jpg\"\n",
    "#datasource = 'tensorflow'\n",
    "#dataset = \"flower_photos\"\n",
    "\n",
    "bronze_format = bronze_config.get('format')\n",
    "bronze_mode = bronze_config.get('mode')\n",
    "\n",
    "print(bronze_format)\n",
    "print(bronze_mode)\n",
    "\n",
    "#format = config.get('source').get('options').get('cloudFiles.format')\n",
    "format = config.get('source').get('format')\n",
    "datasource = config.get('datasource')\n",
    "dataset = config.get('dataset')\n",
    "options = config.get('source').get('options')\n",
    "metadata_columns = config.get('metadata')\n",
    "kafka_config = get_kafka_config(read_config_file('../config/kafka/client.properties'))\n",
    "schema_registry_config = get_schema_registry_config(read_config_file('../config/kafka/schema_registry.properties'))\n",
    "flatten_value = config.get('flatten_value')\n",
    "value_serializer = config.get(\"value_serializer\")\n",
    "key_serializer = config.get(\"key_serializer\")\n",
    "kakfa_value_schema = config.get(\"kafka_value_schema\")\n",
    "partitionColumn = config.get(\"partition\").get(\"column\")\n",
    "#formatted_date_column_params = config.get(\"partition\").get(\"formatted_date_column_params\")\n",
    "\n",
    "print(format)\n",
    "print(datasource)\n",
    "print(dataset)\n",
    "print(options)\n",
    "print(metadata_columns)\n",
    "print(kafka_config)\n",
    "print(flatten_value)\n",
    "print(partitionColumn)\n",
    "print(kakfa_value_schema)\n",
    "#print(formatted_date_column_params)\n",
    "print(schema_registry_config)\n",
    "dataset_landing_path = f\"{landing_path}/{datasource}/{dataset}\"\n",
    "dataset_raw_path =  f\"{raw_path}/{datasource}/{dataset}\"\n",
    "dataset_bronze_path = f\"{bronze_path}/{datasource}/{dataset}\"\n",
    "\n",
    "print(dataset_landing_path)\n",
    "print(dataset_raw_path)\n",
    "print(dataset_bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab982c3-9c97-4df8-adb2-308abcd83ef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reader = (LandingStreamReader.Builder()          \n",
    "  .set_datasource(datasource)\n",
    "  .set_dataset(dataset)\n",
    "  #.set_landing_path(landing_path)\n",
    "  #.set_raw_path(raw_path)\n",
    "  .set_bronze_path(bronze_path)\n",
    "  .set_format(format)\n",
    "  .set_options(options)\n",
    "  #.set_metadata_columns(metadata_columns)\n",
    "  .set_partitionColumn(partitionColumn)\n",
    "  #.set_formatted_date_column_params(formatted_date_column_params)\n",
    "  .set_kafka_config(kafka_config)\n",
    "  .set_schema_registry_config(schema_registry_config)\n",
    "  .set_flatten_value(flatten_value)\n",
    "  .set_value_serializer(value_serializer)\n",
    "  .set_key_serializer(key_serializer)\n",
    "  .set_kafka_value_schema(kakfa_value_schema)\n",
    "  .build()\n",
    ")\n",
    "\n",
    "print(reader)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3074ad0b-e50b-4c3e-a13d-78015a236315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reader.value_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd41f6f-9f6f-4514-8ba0-bc73da084d3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = reader.read_kafka()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a616efe9-912e-4c44-83a1-8ca8c82e66a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2c374c-2a4d-4f5e-81fc-89f14f204454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reader.formatted_date_column_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5814ea1-6b4a-4f32-9e60-a34c9f169fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(reader.options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9861d21f-8321-4d64-9c91-7c09e68c5567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "writer = (BronzeStreamWriter.Builder()\n",
    "  .set_datasource(datasource)\n",
    "  .set_dataset(dataset)\n",
    "  .set_landing_path(landing_path)\n",
    "  .set_raw_path(raw_path)\n",
    "  .set_bronze_path(bronze_path)\n",
    "  .set_bronze_table_format(bronze_format)\n",
    "  .set_bronze_write_mode(bronze_mode)\n",
    "  .set_partitionColumn(partitionColumn)\n",
    "  .build()\n",
    ")\n",
    "\n",
    "print(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7270441-e5cb-438f-b05f-1a214656eb91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "writer.formatted_date_column_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c813aff0-9e16-4404-80eb-7226668bfb6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(reader\n",
    "  .read_kafka()\n",
    "  .writeStream\n",
    "  .foreachBatch(writer.append_2_bronze)\n",
    "  .trigger(availableNow=True)\n",
    "  #.trigger(processingTime=\"60 seconds\") # modo continuo\n",
    "  .option(\"checkpointLocation\", writer.dataset_checkpoint_location)\n",
    "  .queryName(writer.query_name)\n",
    "  .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa024fed-e555-481b-8a45-9714407e6cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select * \n",
    "from delta.`{writer.dataset_bronze_path}`\n",
    "order by _ingested_at desc\n",
    "limit 100\n",
    "\"\"\"\n",
    "display(spark.sql(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2feb764-4ba4-4c67-8d46-82366719adfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select distinct _ingested_filename \n",
    "from delta.`{writer.dataset_bronze_path}`\n",
    "\"\"\"\n",
    "display(spark.sql(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c13a893d-4d08-4e63-8cdb-288ca18913c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestion_engine",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
