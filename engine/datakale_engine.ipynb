{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f280fe21-64d2-4af7-abff-35e0df186152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../imports/imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71fa3335-659a-42c1-aff4-9a05e92bfe71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config/paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a48fb73-c123-4ec0-96cc-5e105d23de9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../utilities/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d2e632-fbcd-4879-9b6e-564921ed029c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(landing_path)\n",
    "print(raw_path)\n",
    "print(bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fecc4502-db01-4d26-84d4-1797c50a0994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class LandingStreamReader:\n",
    "\n",
    "    def __init__(self, builder):\n",
    "        self.datasource = builder.datasource\n",
    "        self.dataset = builder.dataset\n",
    "        self.landing_path = builder.landing_path\n",
    "        self.raw_path = builder.raw_path\n",
    "        self.bronze_path = builder.bronze_path\n",
    "        self.format = builder.format\n",
    "        self.options = builder.options\n",
    "        self.metadata_columns = builder.metadata_columns\n",
    "        self.partitionColumn = builder.partitionColumn\n",
    "        self.formatted_date_column_params = builder.formatted_date_column_params\n",
    "        self.dataset_landing_path = f'{self.landing_path}/{self.datasource}/{self.dataset}'\n",
    "        self.dataset_bronze_schema_location = f'{self.bronze_path}/{self.datasource}/{self.dataset}_schema'\n",
    "        self.kafka_config = builder.kafka_config\n",
    "        self.schema_registry_config = builder.schema_registry_config\n",
    "        self.kafka_value_schema = builder.kafka_value_schema\n",
    "        self.kafka_key_schema = builder.kafka_key_schema\n",
    "        self.flatten_value = builder.flatten_value\n",
    "        self.value_serializer = builder.value_serializer\n",
    "        self.key_serializer = builder.key_serializer\n",
    "\n",
    "        dbutils.fs.mkdirs(self.dataset_bronze_schema_location)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"LandingStreamReader(datasource='{self.datasource}', dataset='{self.dataset}')\"\n",
    "\n",
    "    def read_cloudFiles(self):\n",
    "        df = (spark.readStream\n",
    "              .format(\"cloudFiles\")\n",
    "              .options(**self.options)\n",
    "              .option(\"cloudFiles.schemaLocation\", self.dataset_bronze_schema_location)\n",
    "              .load(self.dataset_landing_path)\n",
    "        )\n",
    "\n",
    "        df = add_metadata_columns(\n",
    "            df,\n",
    "            self.format,\n",
    "            self.landing_path,\n",
    "            self.raw_path,\n",
    "            self.metadata_columns\n",
    "        )\n",
    "\n",
    "        if self.partitionColumn and self.formatted_date_column_params:\n",
    "            df = add_formatted_date_column(df,self.partitionColumn, **self.formatted_date_column_params)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def read_kafka(self):\n",
    "        df=(spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .options(**self.kafka_config)\n",
    "            .options(**self.options)\n",
    "            .load()\n",
    "        )\n",
    "\n",
    "        columns = [F.col(column).alias(f'_{column}') for column in df.columns]\n",
    "        \n",
    "        df = df.select(*columns)\n",
    "\n",
    "        df = deserialize_df(\n",
    "            df = df,\n",
    "            value_serializer=self.value_serializer,\n",
    "            key_serializer=self.key_serializer,\n",
    "            topic = self.dataset,\n",
    "            schema_registry_config = self.schema_registry_config,\n",
    "            flatten_value = self.flatten_value,\n",
    "            value_schema = self.kafka_value_schema,\n",
    "            key_schema = self.kafka_key_schema\n",
    "        ) \n",
    "\n",
    "        df = add_metadata_columns(\n",
    "            df,\n",
    "            format = \"kafka\",\n",
    "        )\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def read(self):\n",
    "        if self.format == \"cloudFiles\":\n",
    "            return self.read_cloudFiles()\n",
    "        elif self.format == \"kafka\":\n",
    "            return self.read_kafka()\n",
    "        else:\n",
    "            raise Exception(f\"Format {self.format} not supported\")\n",
    "\n",
    "    class Builder:\n",
    "        def __init__(self):\n",
    "            self.datasource = None\n",
    "            self.dataset = None\n",
    "            self.landing_path = None\n",
    "            self.raw_path = None\n",
    "            self.bronze_path = None\n",
    "            self.format=None\n",
    "            self.options = None\n",
    "            self.metadata_columns=None\n",
    "            self.partitionColumn = None\n",
    "            self.formatted_date_column_params = None\n",
    "            self.kafka_config = None\n",
    "            self.schema_registry_config = None\n",
    "            self.kafka_value_schema = None\n",
    "            self.kafka_key_schema = None\n",
    "            self.flatten_value = None\n",
    "            self.value_serializer = None\n",
    "            self.key_serializer = None\n",
    "\n",
    "        def set_datasource(self, datasource): #json del dataset\n",
    "            self.datasource = datasource\n",
    "            return self\n",
    "\n",
    "        def set_dataset(self, dataset): #json del dataset\n",
    "            self.dataset = dataset\n",
    "            return self\n",
    "\n",
    "        def set_landing_path(self, landing_path): # variable de entorno\n",
    "            self.landing_path = landing_path\n",
    "            return self\n",
    "\n",
    "        def set_raw_path(self, raw_path): # variable de entorno\n",
    "            self.raw_path = raw_path\n",
    "            return self\n",
    "\n",
    "        def set_bronze_path(self, bronze_path): # variable de entorno\n",
    "            self.bronze_path = bronze_path\n",
    "            return self\n",
    "        \n",
    "        def set_format(self, format): #json del dataset\n",
    "            self.format = format\n",
    "            return self\n",
    "\n",
    "        def set_options(self, options): # json del dataset\n",
    "            self.options = options\n",
    "            return self\n",
    "        \n",
    "        def set_metadata_columns(self, metadata_columns): # json del dataset \n",
    "            self.metadata_columns = metadata_columns\n",
    "            return self\n",
    "        \n",
    "        def set_partitionColumn(self, partitionColumn): #json del dataset \n",
    "            self.partitionColumn = partitionColumn\n",
    "            return self\n",
    "        \n",
    "        def set_formatted_date_column_params(self, formatted_date_column_params): #json del dataset\n",
    "            self.formatted_date_column_params = formatted_date_column_params\n",
    "            return self\n",
    "        \n",
    "        def set_kafka_config(self,kafka_config): # fichero de configuracion de kakfa, client.properties\n",
    "            self.kafka_config = kafka_config\n",
    "            return self\n",
    "        \n",
    "        def set_schema_registry_config(self,schema_registry_config): # fichero de configuracion del esquema registry, schema_registry.properties\n",
    "            self.schema_registry_config = schema_registry_config\n",
    "            return self\n",
    "        \n",
    "        def set_kafka_value_schema(self,kafka_value_schema): #json del dataset\n",
    "            self.kafka_value_schema = kafka_value_schema\n",
    "            return self\n",
    "        \n",
    "        def set_kafka_key_schema(self,kafka_key_schema): #json del dataset \n",
    "            self.kafka_key_schema = kafka_key_schema\n",
    "            return self\n",
    "        \n",
    "        def set_flatten_value(self,flatten_value): #json del dataset\n",
    "            self.flatten_value = flatten_value\n",
    "            return self\n",
    "        \n",
    "        def set_value_serializer(self,value_serializer): #json del dataset \n",
    "            self.value_serializer = value_serializer\n",
    "            return self\n",
    "        \n",
    "        def set_key_serializer(self,key_serializer): # json del dataset\n",
    "            self.key_serializer = key_serializer\n",
    "            return self\n",
    "\n",
    "        def build(self):\n",
    "            return LandingStreamReader(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9295622-bc09-4a5d-b8d8-0b3407c07610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class BronzeStreamWriter:   \n",
    "    def __init__(self, builder):\n",
    "        self.datasource = builder.datasource\n",
    "        self.dataset = builder.dataset\n",
    "        self.landing_path = builder.landing_path\n",
    "        self.raw_path = builder.raw_path\n",
    "        self.bronze_path = builder.bronze_path\n",
    "        self.bronze_table_format = builder.bronze_table_format\n",
    "        self.bronze_write_mode = builder.bronze_write_mode\n",
    "        self.options = builder.options\n",
    "        self.partitionColumn = builder.partitionColumn\n",
    "        self.formatted_date_column_params = builder.formatted_date_column_params\n",
    "        self.dataset_landing_path = f\"{self.landing_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_raw_path = f\"{self.raw_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_bronze_path = f\"{self.bronze_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_checkpoint_location = f\"{self.dataset_bronze_path}_checkpoint\"\n",
    "        self.table = f\"hive_metastore.bronze.{self.datasource}__{self.dataset}\"\n",
    "        self.query_name = f\"bronze-{self.datasource}-{self.dataset}\"\n",
    "\n",
    "        dbutils.fs.mkdirs(self.dataset_raw_path)\n",
    "        dbutils.fs.mkdirs(self.dataset_bronze_path)\n",
    "        dbutils.fs.mkdirs(self.dataset_checkpoint_location)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"BronzeStreamWriter(datasource='{self.datasource}', dataset='{self.dataset}')\"\n",
    "         \n",
    "    def archive_raw_files(self, df):\n",
    "        \"\"\"\n",
    "        Moves ingested raw files from landing to raw path after processing.\n",
    "        \"\"\"\n",
    "        if \"_ingested_filename\" in df.columns:\n",
    "            files = [row[\"_ingested_filename\"] for row in df.select(\"_ingested_filename\").distinct().collect()]\n",
    "            for file in files:\n",
    "                if file:\n",
    "                    file_landing_path = file.replace(self.dataset_raw_path, self.dataset_landing_path)\n",
    "                    dbutils.fs.mkdirs(file[0:file.rfind('/')+1])\n",
    "                    dbutils.fs.mv(file_landing_path, file)\n",
    "    \n",
    "    def write_data(self, df):\n",
    "        \"\"\"\n",
    "        Writes DataFrame to Delta table in bronze layer with schema merge and Delta Lake support.\n",
    "        \"\"\"\n",
    "        spark.sql(\"CREATE DATABASE IF NOT EXISTS hive_metastore.bronze\") \n",
    "        #spark.sql(f\"CREATE TABLE IF NOT EXISTS {self.table} USING DELTA LOCATION '{self.dataset_bronze_path}'\") \n",
    "        \n",
    "        writer = (\n",
    "            df.write\n",
    "           .format(self.bronze_table_format)\n",
    "           .mode(self.bronze_write_mode)\n",
    "           .option(\"mergeSchema\", \"true\")\n",
    "        )\n",
    "\n",
    "        if self.partitionColumn and not self.formatted_date_column_params:\n",
    "            writer.partitionBy(self.partitionColumn)\n",
    "\n",
    "        if self.partitionColumn and self.formatted_date_column_params:\n",
    "            writer.partitionBy(self.formatted_date_column_params[\"output_col\"])\n",
    "        \n",
    "        (writer\n",
    "        .option(\"path\", self.dataset_bronze_path)\n",
    "        .saveAsTable(self.table)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def append_2_bronze(self, batch_df, batch_id):\n",
    "        \"\"\"\n",
    "        Main entrypoint for Structured Streaming write logic.\n",
    "        Persists, writes, archives, and unpersists the batch DataFrame.\n",
    "        \"\"\"\n",
    "        batch_df.persist()\n",
    "        self.write_data(batch_df)\n",
    "        self.archive_raw_files(batch_df)\n",
    "        batch_df.unpersist()\n",
    "\n",
    "    class Builder:\n",
    "        def __init__(self):\n",
    "            self.datasource = None\n",
    "            self.dataset = None\n",
    "            self.landing_path = None\n",
    "            self.raw_path = None\n",
    "            self.bronze_path = None\n",
    "            self.bronze_table_format = None\n",
    "            self.bronze_write_mode = None\n",
    "            self.options = None\n",
    "            self.partitionColumn = None\n",
    "            self.formatted_date_column_params = None\n",
    "        \n",
    "        def set_datasource(self, datasource):\n",
    "            self.datasource = datasource\n",
    "            return self\n",
    "        \n",
    "        def set_dataset(self, dataset):\n",
    "            self.dataset = dataset\n",
    "            return self\n",
    "        \n",
    "        def set_landing_path(self, landing_path):\n",
    "            self.landing_path = landing_path\n",
    "            return self\n",
    "        \n",
    "        def set_raw_path(self, raw_path):\n",
    "            self.raw_path = raw_path\n",
    "            return self\n",
    "        \n",
    "        def set_bronze_path(self, bronze_path):\n",
    "            self.bronze_path = bronze_path\n",
    "            return self\n",
    "        \n",
    "        def set_bronze_table_format(self, bronze_table_format):\n",
    "            self.bronze_table_format = bronze_table_format\n",
    "            return self\n",
    "        \n",
    "        def set_bronze_write_mode(self, bronze_write_mode):\n",
    "            self.bronze_write_mode = bronze_write_mode\n",
    "            return self\n",
    "        \n",
    "        def set_options(self, options):\n",
    "            self.options = options\n",
    "            return self\n",
    "        \n",
    "        def set_partitionColumn(self, partitionColumn):\n",
    "            self.partitionColumn = partitionColumn\n",
    "            return self\n",
    "        \n",
    "        def set_formatted_date_column_params(self, formatted_date_column_params):\n",
    "            self.formatted_date_column_params = formatted_date_column_params\n",
    "            return self\n",
    "        \n",
    "        def build(self):\n",
    "            return BronzeStreamWriter(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77076220-02f1-47fb-8212-f993d7fa0c3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_path = '../config/datasets/batch/orders.json'\n",
    "\n",
    "with open(json_path) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config.get('source').get('options').get('cloudFiles.format')\n",
    "\n",
    "bronze_config_path = '../config/bronze.json'\n",
    "with open(bronze_config_path) as f:\n",
    "    bronze_config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d376ec27-114d-4422-8d59-3ae244cc6362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#format = \"jpg\"\n",
    "#datasource = 'tensorflow'\n",
    "#dataset = \"flower_photos\"\n",
    "\n",
    "bronze_format = bronze_config.get('format')\n",
    "bronze_mode = bronze_config.get('mode')\n",
    "\n",
    "print(bronze_format)\n",
    "print(bronze_mode)\n",
    "\n",
    "#format = config.get('source').get('options').get('cloudFiles.format')\n",
    "format = config.get('source').get('format')\n",
    "datasource = config.get('datasource')\n",
    "dataset = config.get('dataset')\n",
    "options = config.get('source').get('options')\n",
    "metadata_columns = config.get('metadata')\n",
    "kafka_config = get_kafka_config(read_config_file('../config/kafka/client.properties'))\n",
    "schema_registry_config = get_schema_registry_config(read_config_file('../config/kafka/schema_registry.properties'))\n",
    "flatten_value = config.get('flatten_value')\n",
    "value_serializer = config.get(\"value_serializer\")\n",
    "key_serializer = config.get(\"key_serializer\")\n",
    "kakfa_value_schema = config.get(\"kafka_value_schema\")\n",
    "partitionColumn = config.get(\"partition\").get(\"column\")\n",
    "#formatted_date_column_params = config.get(\"partition\").get(\"formatted_date_column_params\")\n",
    "\n",
    "print(format)\n",
    "print(datasource)\n",
    "print(dataset)\n",
    "print(options)\n",
    "print(metadata_columns)\n",
    "print(kafka_config)\n",
    "print(flatten_value)\n",
    "print(partitionColumn)\n",
    "print(kakfa_value_schema)\n",
    "#print(formatted_date_column_params)\n",
    "print(schema_registry_config)\n",
    "dataset_landing_path = f\"{landing_path}/{datasource}/{dataset}\"\n",
    "dataset_raw_path =  f\"{raw_path}/{datasource}/{dataset}\"\n",
    "dataset_bronze_path = f\"{bronze_path}/{datasource}/{dataset}\"\n",
    "\n",
    "print(dataset_landing_path)\n",
    "print(dataset_raw_path)\n",
    "print(dataset_bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab982c3-9c97-4df8-adb2-308abcd83ef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reader = (LandingStreamReader.Builder()          \n",
    "  .set_datasource(datasource)\n",
    "  .set_dataset(dataset)\n",
    "  #.set_landing_path(landing_path)\n",
    "  #.set_raw_path(raw_path)\n",
    "  .set_bronze_path(bronze_path)\n",
    "  .set_format(format)\n",
    "  .set_options(options)\n",
    "  #.set_metadata_columns(metadata_columns)\n",
    "  .set_partitionColumn(partitionColumn)\n",
    "  #.set_formatted_date_column_params(formatted_date_column_params)\n",
    "  .set_kafka_config(kafka_config)\n",
    "  .set_schema_registry_config(schema_registry_config)\n",
    "  .set_flatten_value(flatten_value)\n",
    "  .set_value_serializer(value_serializer)\n",
    "  .set_key_serializer(key_serializer)\n",
    "  .set_kafka_value_schema(kakfa_value_schema)\n",
    "  .build()\n",
    ")\n",
    "\n",
    "print(reader)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3074ad0b-e50b-4c3e-a13d-78015a236315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reader.value_serializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd41f6f-9f6f-4514-8ba0-bc73da084d3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = reader.read_kafka()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a616efe9-912e-4c44-83a1-8ca8c82e66a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2c374c-2a4d-4f5e-81fc-89f14f204454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reader.formatted_date_column_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5814ea1-6b4a-4f32-9e60-a34c9f169fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(reader.options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9861d21f-8321-4d64-9c91-7c09e68c5567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "writer = (BronzeStreamWriter.Builder()\n",
    "  .set_datasource(datasource)\n",
    "  .set_dataset(dataset)\n",
    "  .set_landing_path(landing_path)\n",
    "  .set_raw_path(raw_path)\n",
    "  .set_bronze_path(bronze_path)\n",
    "  .set_bronze_table_format(bronze_format)\n",
    "  .set_bronze_write_mode(bronze_mode)\n",
    "  .set_partitionColumn(partitionColumn)\n",
    "  #.set_formatted_date_column_params(formatted_date_column_params)\n",
    "  .build()\n",
    ")\n",
    "\n",
    "print(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7270441-e5cb-438f-b05f-1a214656eb91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "writer.formatted_date_column_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c813aff0-9e16-4404-80eb-7226668bfb6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(reader\n",
    "  .read_kafka()\n",
    "  .writeStream\n",
    "  .foreachBatch(writer.append_2_bronze)\n",
    "  .trigger(availableNow=True)\n",
    "  #.trigger(processingTime=\"60 seconds\") # modo continuo\n",
    "  .option(\"checkpointLocation\", writer.dataset_checkpoint_location)\n",
    "  .queryName(writer.query_name)\n",
    "  .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa024fed-e555-481b-8a45-9714407e6cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select * \n",
    "from delta.`{writer.dataset_bronze_path}`\n",
    "order by _ingested_at desc\n",
    "limit 100\n",
    "\"\"\"\n",
    "display(spark.sql(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2feb764-4ba4-4c67-8d46-82366719adfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select distinct _ingested_filename \n",
    "from delta.`{writer.dataset_bronze_path}`\n",
    "\"\"\"\n",
    "display(spark.sql(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c13a893d-4d08-4e63-8cdb-288ca18913c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "datakale_engine",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
