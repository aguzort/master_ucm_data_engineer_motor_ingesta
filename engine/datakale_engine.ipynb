{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f280fe21-64d2-4af7-abff-35e0df186152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../imports/imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71fa3335-659a-42c1-aff4-9a05e92bfe71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config/paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a48fb73-c123-4ec0-96cc-5e105d23de9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../utilities/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5d2e632-fbcd-4879-9b6e-564921ed029c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(landing_path)\n",
    "print(raw_path)\n",
    "print(bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fecc4502-db01-4d26-84d4-1797c50a0994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class LandingStreamReader:\n",
    "\n",
    "    IMAGE_EXTENSIONS = [\n",
    "        \"jpg\", \"jpeg\", \"png\", \"bmp\", \"gif\", \"tiff\", \"tif\",\n",
    "        \"webp\", \"heic\", \"heif\", \"ico\", \"svg\", \"raw\", \"cr2\",\n",
    "        \"nef\", \"orf\", \"arw\", \"psd\", \"indd\", \"ai\", \"eps\"\n",
    "    ]\n",
    "\n",
    "    IMAGE_GENERAL_KEYWORD = \"image\"\n",
    "\n",
    "    def __init__(self, builder):\n",
    "        self.datasource = builder.datasource\n",
    "        self.dataset = builder.dataset\n",
    "        self.landing_path = builder.landing_path\n",
    "        self.raw_path = builder.raw_path\n",
    "        self.bronze_path = builder.bronze_path\n",
    "        self.format = builder.format\n",
    "        self.dataset_landing_path = f'{self.landing_path}/{self.datasource}/{self.dataset}'\n",
    "        self.dataset_bronze_schema_location = f'{self.bronze_path}/{self.datasource}/{self.dataset}_schema'\n",
    "        dbutils.fs.mkdirs(self.dataset_bronze_schema_location)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"LandingStreamReader(datasource='{self.datasource}', dataset='{self.dataset}')\"\n",
    "\n",
    "    def read_json(self):\n",
    "        return (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"json\")\n",
    "                .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "                .option(\"cloudFiles.schemaLocation\", self.dataset_bronze_schema_location)\n",
    "                .load(self.dataset_landing_path))\n",
    "\n",
    "    def read_binary(self):\n",
    "        return (spark.readStream\n",
    "                .format(\"cloudFiles\")\n",
    "                .option(\"cloudFiles.format\", \"binaryFile\")\n",
    "                .option(\"recursiveFileLookup\", \"true\")\n",
    "                .option(\"cloudFiles.schemaLocation\", self.dataset_bronze_schema_location)\n",
    "                .load(self.dataset_landing_path))\n",
    "\n",
    "    def read(self):\n",
    "        match self.format:\n",
    "            case \"json\":\n",
    "                df = self.read_json()\n",
    "            case x if x in LandingStreamReader.IMAGE_EXTENSIONS or x == LandingStreamReader.IMAGE_GENERAL_KEYWORD:\n",
    "                df = self.read_binary()\n",
    "            case _:\n",
    "                raise Exception(f\"Format {self.format} not supported\")\n",
    "\n",
    "        return add_metadata_columns(\n",
    "            df,\n",
    "            self.landing_path,\n",
    "            self.raw_path,\n",
    "            self.format,\n",
    "            LandingStreamReader.IMAGE_EXTENSIONS,\n",
    "            LandingStreamReader.IMAGE_GENERAL_KEYWORD\n",
    "        )\n",
    "\n",
    "    class Builder:\n",
    "        def __init__(self):\n",
    "            self.datasource = None\n",
    "            self.dataset = None\n",
    "            self.landing_path = None\n",
    "            self.raw_path = None\n",
    "            self.bronze_path = None\n",
    "            self.format = None\n",
    "\n",
    "        def set_datasource(self, datasource):\n",
    "            self.datasource = datasource\n",
    "            return self\n",
    "\n",
    "        def set_dataset(self, dataset):\n",
    "            self.dataset = dataset\n",
    "            return self\n",
    "\n",
    "        def set_landing_path(self, landing_path):\n",
    "            self.landing_path = landing_path\n",
    "            return self\n",
    "\n",
    "        def set_raw_path(self, raw_path):\n",
    "            self.raw_path = raw_path\n",
    "            return self\n",
    "\n",
    "        def set_bronze_path(self, bronze_path):\n",
    "            self.bronze_path = bronze_path\n",
    "            return self\n",
    "\n",
    "        def set_format(self, format):\n",
    "            self.format = format\n",
    "            return self\n",
    "\n",
    "        def build(self):\n",
    "            return LandingStreamReader(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b9295622-bc09-4a5d-b8d8-0b3407c07610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class BronzeStreamWriter:   \n",
    "    def __init__(self, builder):\n",
    "        self.datasource = builder.datasource\n",
    "        self.dataset = builder.dataset\n",
    "        self.landing_path = builder.landing_path\n",
    "        self.raw_path = builder.raw_path\n",
    "        self.bronze_path = builder.bronze_path\n",
    "\n",
    "        self.dataset_landing_path = f\"{self.landing_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_raw_path = f\"{self.raw_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_bronze_path = f\"{self.bronze_path}/{self.datasource}/{self.dataset}\"\n",
    "        self.dataset_checkpoint_location = f\"{self.dataset_bronze_path}_checkpoint\"\n",
    "        self.table = f\"hive_metastore.bronze.{self.datasource}_{self.dataset}\"\n",
    "        self.query_name = f\"bronze-{self.datasource}-{self.dataset}\"\n",
    "\n",
    "        dbutils.fs.mkdirs(self.dataset_raw_path)\n",
    "        dbutils.fs.mkdirs(self.dataset_bronze_path)\n",
    "        dbutils.fs.mkdirs(self.dataset_checkpoint_location)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"BronzeStreamWriter(datasource='{self.datasource}', dataset='{self.dataset}')\"\n",
    "         \n",
    "    def archive_raw_files(self, df):\n",
    "        \"\"\"\n",
    "        Moves ingested raw files from landing to raw path after processing.\n",
    "        \"\"\"\n",
    "        if \"_ingested_filename\" in df.columns:\n",
    "            files = [row[\"_ingested_filename\"] for row in df.select(\"_ingested_filename\").distinct().collect()]\n",
    "            for file in files:\n",
    "                if file:\n",
    "                    file_landing_path = file.replace(self.dataset_raw_path, self.dataset_landing_path)\n",
    "                    dbutils.fs.mkdirs(file[0:file.rfind('/')+1])\n",
    "                    dbutils.fs.mv(file_landing_path, file)\n",
    "    \n",
    "    def write_data(self, df):\n",
    "        \"\"\"\n",
    "        Writes DataFrame to Delta table in bronze layer with schema merge and Delta Lake support.\n",
    "        \"\"\"\n",
    "        spark.sql(\"CREATE DATABASE IF NOT EXISTS hive_metastore.bronze\") \n",
    "        spark.sql(f\"CREATE TABLE IF NOT EXISTS {self.table} USING DELTA LOCATION '{self.dataset_bronze_path}'\") \n",
    "        \n",
    "        (df.write\n",
    "           .format(\"delta\")\n",
    "           .mode(\"append\")\n",
    "           .option(\"mergeSchema\", \"true\")\n",
    "           .option(\"path\", self.dataset_bronze_path)\n",
    "           .saveAsTable(self.table)\n",
    "        )\n",
    "        \n",
    "    def append_2_bronze(self, batch_df, batch_id):\n",
    "        \"\"\"\n",
    "        Main entrypoint for Structured Streaming write logic.\n",
    "        Persists, writes, archives, and unpersists the batch DataFrame.\n",
    "        \"\"\"\n",
    "        batch_df.persist()\n",
    "        self.write_data(batch_df)\n",
    "        self.archive_raw_files(batch_df)\n",
    "        batch_df.unpersist()\n",
    "\n",
    "    class Builder:\n",
    "        def __init__(self):\n",
    "            self.datasource = None\n",
    "            self.dataset = None\n",
    "            self.landing_path = None\n",
    "            self.raw_path = None\n",
    "            self.bronze_path = None\n",
    "        \n",
    "        def set_datasource(self, datasource):\n",
    "            self.datasource = datasource\n",
    "            return self\n",
    "        \n",
    "        def set_dataset(self, dataset):\n",
    "            self.dataset = dataset\n",
    "            return self\n",
    "        \n",
    "        def set_landing_path(self, landing_path):\n",
    "            self.landing_path = landing_path\n",
    "            return self\n",
    "        \n",
    "        def set_raw_path(self, raw_path):\n",
    "            self.raw_path = raw_path\n",
    "            return self\n",
    "        \n",
    "        def set_bronze_path(self, bronze_path):\n",
    "            self.bronze_path = bronze_path\n",
    "            return self\n",
    "        \n",
    "        def build(self):\n",
    "            return BronzeStreamWriter(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d376ec27-114d-4422-8d59-3ae244cc6362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "format = \"jpg\"\n",
    "datasource = 'tensorflow'\n",
    "dataset = \"flower_photos\"\n",
    "\n",
    "#format = \"json\"\n",
    "#datasource = 'retail'\n",
    "#dataset = \"sales_orders\"\n",
    "\n",
    "dataset_landing_path = f\"{landing_path}/{datasource}/{dataset}\"\n",
    "dataset_raw_path =  f\"{raw_path}/{datasource}/{dataset}\"\n",
    "dataset_bronze_path = f\"{bronze_path}/{datasource}/{dataset}\"\n",
    "\n",
    "print(dataset_landing_path)\n",
    "print(dataset_raw_path)\n",
    "print(dataset_bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab982c3-9c97-4df8-adb2-308abcd83ef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reader = (LandingStreamReader.Builder()          \n",
    "  .set_datasource(datasource)\n",
    "  .set_dataset(dataset)\n",
    "  .set_landing_path(landing_path)\n",
    "  .set_raw_path(raw_path)\n",
    "  .set_bronze_path(bronze_path)\n",
    "  .set_format(format)\n",
    "  .build()\n",
    ")\n",
    "\n",
    "print(reader)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9861d21f-8321-4d64-9c91-7c09e68c5567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "writer = (BronzeStreamWriter.Builder()\n",
    "  .set_datasource(datasource)\n",
    "  .set_dataset(dataset)\n",
    "  .set_landing_path(landing_path)\n",
    "  .set_raw_path(raw_path)\n",
    "  .set_bronze_path(bronze_path)\n",
    "  .build()\n",
    ")\n",
    "\n",
    "print(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c813aff0-9e16-4404-80eb-7226668bfb6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(reader\n",
    "  .read()\n",
    "  .writeStream\n",
    "  .foreachBatch(writer.append_2_bronze)\n",
    "  .trigger(availableNow=True)\n",
    "  #.trigger(processingTime=\"60 seconds\") # modo continuo\n",
    "  .option(\"checkpointLocation\", writer.dataset_checkpoint_location)\n",
    "  .queryName(writer.query_name)\n",
    "  .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa024fed-e555-481b-8a45-9714407e6cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select * \n",
    "from delta.`{writer.dataset_bronze_path}`\n",
    "order by _ingested_at desc\n",
    "limit 10\n",
    "\"\"\"\n",
    "display(spark.sql(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2feb764-4ba4-4c67-8d46-82366719adfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "select distinct _ingested_filename \n",
    "from delta.`{writer.dataset_bronze_path}`\n",
    "\"\"\"\n",
    "display(spark.sql(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c13a893d-4d08-4e63-8cdb-288ca18913c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "datakale_engine",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
