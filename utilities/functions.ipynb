{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2851c15b-25ad-4c00-9cb4-6ad00705e44d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../imports/imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "281924c8-caf7-420b-8f82-957655260dcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_all_files(path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively lists all files under a given directory path using dbutils.fs.\n",
    "\n",
    "    Args:\n",
    "        path (str): Base path in the Databricks file system (e.g., 'dbfs:/mnt/my_folder').\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of full paths to all files found within the directory tree.\n",
    "    \"\"\"\n",
    "    files_list: List[str] = []\n",
    "\n",
    "    try:\n",
    "        # List contents of the current path\n",
    "        items = dbutils.fs.ls(path)\n",
    "\n",
    "        for item in items:\n",
    "            if item.isFile():\n",
    "                # Append file path to the result list\n",
    "                files_list.append(item.path)\n",
    "            elif item.isDir():\n",
    "                # Recursively explore subdirectories\n",
    "                files_list.extend(list_all_files(item.path))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while listing {path}: {str(e)}\")\n",
    "\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f5d9df-de70-4372-9986-898d7f565cfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_size(content: bytes) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Extract the (width, height) from binary image content.\n",
    "\n",
    "    Args:\n",
    "        content (bytes): The binary content of the image.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (width, height) of the image.\n",
    "    \"\"\"\n",
    "    image = Image.open(io.BytesIO(content))\n",
    "    return image.size\n",
    "\n",
    "\n",
    "@pandas_udf(\"width: int, height: int\")\n",
    "def extract_size_udf(content_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pandas UDF to extract image dimensions (width, height) from a column of binary content.\n",
    "\n",
    "    Args:\n",
    "        content_series (pd.Series): A Pandas Series of binary image contents.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with 'width' and 'height' columns.\n",
    "    \"\"\"\n",
    "    sizes = content_series.apply(extract_size)\n",
    "    return pd.DataFrame(list(sizes))\n",
    "\n",
    "\n",
    "def extract_label(path_col: Column) -> Column:\n",
    "    \"\"\"\n",
    "    Extract label from a path using a regular expression.\n",
    "\n",
    "    Args:\n",
    "        path_col (Column): Spark column containing the file path.\n",
    "\n",
    "    Returns:\n",
    "        Column: A new column containing the extracted label.\n",
    "    \"\"\"\n",
    "    return F.regexp_extract(path_col, r\"flower_photos/([^/]+)\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77bfe6b6-2897-4312-9041-bbc74234dcbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_metadata_columns(\n",
    "    df: SparkDataFrame,\n",
    "    format: str = 'cloudFiles',\n",
    "    landing_path: Optional[str]=None,\n",
    "    raw_path: Optional[str]= None,\n",
    "    metadata_columns: Optional[List[str]] = None,\n",
    "    column_for_size: str = \"content\",\n",
    "    column_for_label :str = \"path\"\n",
    ") -> SparkDataFrame:\n",
    "    \"\"\"\n",
    "    Add standard metadata columns to a Spark DataFrame, including ingestion time and filename.\n",
    "    If the data contains images, also adds image size and label columns.\n",
    "\n",
    "    Args:\n",
    "        df (SparkDataFrame): The input Spark DataFrame.\n",
    "        landing_path (str): Original path where the data landed.\n",
    "        raw_path (str): Destination path in the bronze layer.\n",
    "        format (str): Format of the dataset (e.g., \"json\", \"image\").\n",
    "        image_extensions (List[str]): List of recognized image file extensions.\n",
    "        image_keyword (str): Keyword used to identify image format (e.g., \"image\").\n",
    "\n",
    "    Returns:\n",
    "        SparkDataFrame: Spark DataFrame with added metadata columns.\n",
    "    \"\"\"\n",
    "    if metadata_columns is None:\n",
    "        metadata_columns = [\"default\"]\n",
    "        \n",
    "    data_cols = df.columns\n",
    "\n",
    "    metadata_cols: dict[str, Column] = {}\n",
    "\n",
    "    if format == 'cloudFiles':\n",
    "\n",
    "        if \"default\" in metadata_columns or \"ingested_at\" in metadata_columns:\n",
    "            metadata_cols[\"_ingested_at\"] = F.current_timestamp()\n",
    "        \n",
    "        if \"default\" in metadata_columns or \"ingested_filename\" in metadata_columns:\n",
    "            metadata_cols[\"_ingested_filename\"] = F.replace(\n",
    "                F.input_file_name(),\n",
    "                F.lit(landing_path),\n",
    "                F.lit(raw_path)\n",
    "            )\n",
    "\n",
    "        if \"size\" in metadata_columns:\n",
    "            metadata_cols[\"_size\"] = extract_size_udf(F.col(column_for_size))\n",
    "\n",
    "        if \"label\" in metadata_columns:\n",
    "            metadata_cols[\"_label\"] = extract_label(F.col(column_for_label))\n",
    "\n",
    "    if format == 'kafka':\n",
    "         metadata_cols[\"_ingested_at\"] = F.current_timestamp()\n",
    "\n",
    "    for col_name, expr in metadata_cols.items():\n",
    "        df = df.withColumn(col_name, expr)\n",
    "\n",
    "    return df.select(list(metadata_cols.keys()) + [c for c in data_cols if c not in metadata_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1920a29d-06d1-475b-9832-d2428e735f35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_formatted_date_column(\n",
    "    df: SparkDataFrame,\n",
    "    input_col: str,\n",
    "    output_col: str = \"formatted_date\",\n",
    "    input_type: Literal[\"unix\", \"unix_millis\", \"string\", \"timestamp\"] = \"timestamp\",\n",
    "    input_format: Optional[str] = None,\n",
    "    output_format: str = \"yyyy-MM\"\n",
    ") -> SparkDataFrame:\n",
    "    \"\"\"\n",
    "    Adds a new column with a formatted date string from a given date column.\n",
    "\n",
    "    Args:\n",
    "        df (SparkDataFrame): Input DataFrame.\n",
    "        input_col (str): Name of the input date column.\n",
    "        output_col (str): Name of the output formatted column.\n",
    "        input_type (str): Type of the input column ('unix', 'unix_millis', 'string', 'timestamp').\n",
    "        input_format (str, optional): Format of the input date string (required if input_type='string').\n",
    "        output_format (str): Desired output date format (Spark-compatible).\n",
    "\n",
    "    Returns:\n",
    "        SparkDataFrame: DataFrame with the new formatted date column.\n",
    "    \"\"\"\n",
    "    if input_type == \"unix\":\n",
    "        timestamp_col = F.from_unixtime(F.col(input_col))\n",
    "    elif input_type == \"unix_millis\":\n",
    "        timestamp_col = F.from_unixtime((F.col(input_col) / 1000).cast(\"long\"))\n",
    "    elif input_type == \"string\":\n",
    "        if not input_format:\n",
    "            raise ValueError(\"You must provide 'input_format' when 'input_type' is 'string'\")\n",
    "        timestamp_col = F.to_timestamp(F.col(input_col), input_format)\n",
    "    elif input_type == \"timestamp\":\n",
    "        timestamp_col = F.col(input_col)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported input_type: {input_type}\")\n",
    "\n",
    "    return df.withColumn(output_col, F.date_format(timestamp_col, output_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0330c4d0-5a64-4ac7-b12b-020b19a4a3e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_config_file(path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Reads a configuration file from the given path and returns its contents as a dictionary.\n",
    "\n",
    "    The function skips empty lines and lines beginning with '#'.\n",
    "    Each valid line should contain a key-value pair separated by '='.\n",
    "    Leading and trailing whitespaces are stripped from both keys and values.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the configuration file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary containing configuration parameters\n",
    "                        as keys and their corresponding values as strings.\n",
    "    \"\"\"\n",
    "    config: Dict[str, str] = {}\n",
    "    with open(path, 'r') as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith(\"#\"):\n",
    "                parameter, value = line.split('=', 1)\n",
    "                config[parameter.strip()] = value.strip()\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da63a9d-27e2-495c-8d63-1e565eec4c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_kafka_config(kafka_properties: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Builds a Kafka client configuration dictionary from the provided input properties.\n",
    "\n",
    "    This function extracts required Kafka connection parameters and formats them\n",
    "    into a configuration dictionary suitable for use with Kafka clients that use SASL authentication.\n",
    "\n",
    "    Required keys in `kafka_properties`:\n",
    "        - 'bootstrap.servers': Kafka broker address.\n",
    "        - 'security.protocol': Security protocol to use (e.g., 'SASL_SSL').\n",
    "        - 'sasl.mechanisms': SASL mechanism (e.g., 'PLAIN').\n",
    "        - 'sasl.username': Username for SASL authentication.\n",
    "        - 'sasl.password': Password for SASL authentication.\n",
    "\n",
    "    Args:\n",
    "        kafka_properties (Dict[str, str]): Dictionary containing Kafka connection settings.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary with keys formatted for use with Kafka clients.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If any of the required keys are missing from the input dictionary.\n",
    "    \"\"\"\n",
    "    kafka_options = {\n",
    "        \"kafka.bootstrap.servers\": kafka_properties[\"bootstrap.servers\"],\n",
    "        \"kafka.security.protocol\": kafka_properties[\"security.protocol\"],\n",
    "        \"kafka.sasl.mechanism\": kafka_properties[\"sasl.mechanisms\"],\n",
    "        \"kafka.sasl.jaas.config\": (\n",
    "            f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required '\n",
    "            f'username=\"{kafka_properties[\"sasl.username\"]}\" '\n",
    "            f'password=\"{kafka_properties[\"sasl.password\"]}\";'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return kafka_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "593f93cc-7c2c-4637-9663-b62926a0c9f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_schema_registry_config(schema_registry_properties: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Validates and builds the configuration dictionary required to connect to a Schema Registry.\n",
    "\n",
    "    This function expects a dictionary containing the following keys:\n",
    "      - 'schema_registry_username'\n",
    "      - 'schema_registry_password'\n",
    "      - 'schema_registry_url'\n",
    "\n",
    "    It validates that these keys are present and returns a new dictionary formatted\n",
    "    for use with Kafka clients (e.g., Confluent Kafka) that require Schema Registry authentication.\n",
    "\n",
    "    Args:\n",
    "        schema_registry_properties (Dict[str, str]): \n",
    "            A dictionary containing the Schema Registry connection parameters.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary with the keys 'url' and 'basic.auth.user.info' \n",
    "                        formatted for use in Kafka client configuration.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the required keys are missing from the input dictionary.\n",
    "    \"\"\"\n",
    "    required_keys = ['schema_registry_username', 'schema_registry_password', 'schema_registry_url']\n",
    "\n",
    "    missing_keys = [key for key in required_keys if key not in schema_registry_properties]\n",
    "\n",
    "    if missing_keys:\n",
    "        raise ValueError(\n",
    "            \"schema_registry_username, schema_registry_password, and schema_registry_url \"\n",
    "            \"keys are required to be defined in the input dictionary\"\n",
    "        )\n",
    "    \n",
    "    schema_registry_config = {\n",
    "        'url': schema_registry_properties['schema_registry_url'],\n",
    "        'basic.auth.user.info': f\"{schema_registry_properties['schema_registry_username']}:{schema_registry_properties['schema_registry_password']}\"\n",
    "    }\n",
    "\n",
    "    return schema_registry_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f8e1381-59ce-4dd1-94d2-dd311f41bde8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def deserialize_df(\n",
    "    df: SparkDataFrame,\n",
    "    topic: str,\n",
    "    value_serializer: str = None,\n",
    "    key_serializer: str = None,\n",
    "    schema_registry_config: Optional[Dict[str, str]] = None,\n",
    "    flatten_value: bool = True,\n",
    "    value_schema: Optional[str] = None,\n",
    "    key_schema: Optional[str] = None\n",
    "    ) -> SparkDataFrame:\n",
    "    \n",
    "    deserialized_columns = {}\n",
    "\n",
    "    if schema_registry_config:\n",
    "        schema_registry_client = SchemaRegistryClient(schema_registry_config)\n",
    "    \n",
    "    if value_serializer == 'avro':\n",
    "        value_subject = f\"{topic}-value\"\n",
    "        value_schema = schema_registry_client.get_latest_version(value_subject).schema.schema_str \n",
    "        deserialized_columns['value'] = from_avro(F.expr(\"substring(_value,6,length(_value)-5)\"),value_schema)\n",
    "\n",
    "    if key_serializer == 'avro':\n",
    "        key_subject = f\"{topic}-key\"\n",
    "        key_schema = schema_registry_client.get_latest_version(key_subject)\n",
    "        deserialized_columns['key'] = from_avro(F.expr(\"substring(_value,6,length(_key)-5)\"),key_schema)\n",
    "\n",
    "    if value_serializer == 'json':\n",
    "        deserialized_columns['value'] = F.from_json(F.col(\"_value\").cast(\"string\"),value_schema)\n",
    "\n",
    "    if key_serializer == 'json':\n",
    "        deserialized_columns['key'] = F.from_json(F.col(\"_value\").cast(\"string\"),key_schema)\n",
    "\n",
    "    if value_serializer =='str':\n",
    "        deserialized_columns['value'] = F.col(\"_value\").cast(\"string\")\n",
    "\n",
    "    if key_serializer == 'str':\n",
    "        deserialized_columns['key'] = F.col(\"_key\").cast(\"string\")\n",
    "\n",
    "    for col_name, expression in deserialized_columns.items():\n",
    "        df = df.withColumn(col_name, expression)\n",
    "\n",
    "    if flatten_value:\n",
    "        df = (df\n",
    "              .select(\"*\", \"value.*\")\n",
    "              .drop(\"value\")\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7e4af2f-d867-4081-ba56-adbb8cf43ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
