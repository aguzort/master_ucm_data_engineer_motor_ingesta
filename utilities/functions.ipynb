{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2851c15b-25ad-4c00-9cb4-6ad00705e44d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../imports/imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "281924c8-caf7-420b-8f82-957655260dcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_all_files(path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively lists all files under a given directory path using dbutils.fs.\n",
    "\n",
    "    Args:\n",
    "        path (str): Base path in the Databricks file system (e.g., 'dbfs:/mnt/my_folder').\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of full paths to all files found within the directory tree.\n",
    "    \"\"\"\n",
    "    files_list: List[str] = []\n",
    "\n",
    "    try:\n",
    "        # List contents of the current path\n",
    "        items = dbutils.fs.ls(path)\n",
    "\n",
    "        for item in items:\n",
    "            if item.isFile():\n",
    "                # Append file path to the result list\n",
    "                files_list.append(item.path)\n",
    "            elif item.isDir():\n",
    "                # Recursively explore subdirectories\n",
    "                files_list.extend(list_all_files(item.path))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while listing {path}: {str(e)}\")\n",
    "\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f5d9df-de70-4372-9986-898d7f565cfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_size(content: bytes) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Extract the (width, height) from binary image content.\n",
    "\n",
    "    Args:\n",
    "        content (bytes): The binary content of the image.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (width, height) of the image.\n",
    "    \"\"\"\n",
    "    image = Image.open(io.BytesIO(content))\n",
    "    return image.size\n",
    "\n",
    "\n",
    "@pandas_udf(\"width: int, height: int\")\n",
    "def extract_size_udf(content_series: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pandas UDF to extract image dimensions (width, height) from a column of binary content.\n",
    "\n",
    "    Args:\n",
    "        content_series (pd.Series): A Pandas Series of binary image contents.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with 'width' and 'height' columns.\n",
    "    \"\"\"\n",
    "    sizes = content_series.apply(extract_size)\n",
    "    return pd.DataFrame(list(sizes))\n",
    "\n",
    "\n",
    "def extract_label(path_col: Column) -> Column:\n",
    "    \"\"\"\n",
    "    Extract label from a path using a regular expression.\n",
    "\n",
    "    Args:\n",
    "        path_col (Column): Spark column containing the file path.\n",
    "\n",
    "    Returns:\n",
    "        Column: A new column containing the extracted label.\n",
    "    \"\"\"\n",
    "    return F.regexp_extract(path_col, r\"flower_photos/([^/]+)\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77bfe6b6-2897-4312-9041-bbc74234dcbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_metadata_columns(\n",
    "    df: SparkDataFrame,\n",
    "    landing_path: str,\n",
    "    raw_path: str,\n",
    "    format: str,\n",
    "    image_extensions: List[str] = None,\n",
    "    image_keyword: str = None\n",
    ") -> SparkDataFrame:\n",
    "    \"\"\"\n",
    "    Add standard metadata columns to a Spark DataFrame, including ingestion time and filename.\n",
    "    If the data contains images, also adds image size and label columns.\n",
    "\n",
    "    Args:\n",
    "        df (SparkDataFrame): The input Spark DataFrame.\n",
    "        landing_path (str): Original path where the data landed.\n",
    "        raw_path (str): Destination path in the bronze layer.\n",
    "        format (str): Format of the dataset (e.g., \"json\", \"image\").\n",
    "        image_extensions (List[str]): List of recognized image file extensions.\n",
    "        image_keyword (str): Keyword used to identify image format (e.g., \"image\").\n",
    "\n",
    "    Returns:\n",
    "        SparkDataFrame: Spark DataFrame with added metadata columns.\n",
    "    \"\"\"\n",
    "    data_cols = df.columns\n",
    "\n",
    "    metadata_cols: dict[str, Column] = {\n",
    "        \"_ingested_at\": F.current_timestamp(),\n",
    "        \"_ingested_filename\": F.replace(\n",
    "            F.input_file_name(),\n",
    "            F.lit(landing_path),\n",
    "            F.lit(raw_path)\n",
    "        )\n",
    "    }\n",
    "\n",
    "    if format in image_extensions or format == image_keyword:\n",
    "        metadata_cols.update({\n",
    "            \"_size\": extract_size_udf(F.col(\"content\")),\n",
    "            \"_label\": extract_label(F.col(\"path\")),\n",
    "        })\n",
    "\n",
    "    for col_name, expr in metadata_cols.items():\n",
    "        df = df.withColumn(col_name, expr)\n",
    "\n",
    "    return df.select(list(metadata_cols.keys()) + [c for c in data_cols if c not in metadata_cols])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
